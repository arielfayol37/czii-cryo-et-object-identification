{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ariel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Library imports\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import sys\n",
    "import zarr\n",
    "import json\n",
    "import os\n",
    "import napari\n",
    "from rich import print as rprint  # Import rich's print function\n",
    "import copy\n",
    "from dataclasses import dataclass\n",
    "from monai.networks.nets import UNet\n",
    "# set torch and cuda seed for reproducibility\n",
    "torch.manual_seed(37)\n",
    "torch.cuda.manual_seed(37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in the model: 1.12e+06\n"
     ]
    }
   ],
   "source": [
    "# Create UNet, DiceLoss and Adam optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNet(\n",
    "    spatial_dims=3,\n",
    "    in_channels=1,\n",
    "    out_channels=6+1,\n",
    "    channels=(48, 64, 80, 80),\n",
    "    strides=(2, 2, 1),\n",
    "    num_res_units=1,\n",
    ").to(device)\n",
    "\n",
    "# print the number of parameters in the model\n",
    "print(f\"Number of parameters in the model: {sum(p.numel() for p in model.parameters()):.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cryoet_data_portal in c:\\users\\ariel\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.0.0)\n",
      "Requirement already satisfied: requests in c:\\users\\ariel\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from cryoet_data_portal) (2.32.3)\n",
      "Requirement already satisfied: boto3 in c:\\users\\ariel\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from cryoet_data_portal) (1.35.0)\n",
      "Requirement already satisfied: deepmerge in c:\\users\\ariel\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from cryoet_data_portal) (2.0)\n",
      "Requirement already satisfied: gql[requests] in c:\\users\\ariel\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from cryoet_data_portal) (3.5.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ariel\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from cryoet_data_portal) (4.66.5)\n",
      "Requirement already satisfied: strcase in c:\\users\\ariel\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from cryoet_data_portal) (1.0.0)\n",
      "Requirement already satisfied: botocore<1.36.0,>=1.35.0 in c:\\users\\ariel\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from boto3->cryoet_data_portal) (1.35.93)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\ariel\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from boto3->cryoet_data_portal) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in c:\\users\\ariel\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from boto3->cryoet_data_portal) (0.10.3)\n",
      "Requirement already satisfied: graphql-core<3.3,>=3.2 in c:\\users\\ariel\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gql[requests]->cryoet_data_portal) (3.2.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.6 in c:\\users\\ariel\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gql[requests]->cryoet_data_portal) (1.15.4)\n",
      "Requirement already satisfied: backoff<3.0,>=1.11.1 in c:\\users\\ariel\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gql[requests]->cryoet_data_portal) (2.2.1)\n",
      "Requirement already satisfied: anyio<5,>=3.0 in c:\\users\\ariel\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gql[requests]->cryoet_data_portal) (4.6.2.post1)\n",
      "Requirement already satisfied: requests-toolbelt<2,>=1.0.0 in c:\\users\\ariel\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gql[requests]->cryoet_data_portal) (1.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ariel\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->cryoet_data_portal) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ariel\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->cryoet_data_portal) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ariel\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->cryoet_data_portal) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ariel\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->cryoet_data_portal) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\ariel\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm->cryoet_data_portal) (0.4.6)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\ariel\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from anyio<5,>=3.0->gql[requests]->cryoet_data_portal) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\ariel\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from botocore<1.36.0,>=1.35.0->boto3->cryoet_data_portal) (2.8.2)\n",
      "Requirement already satisfied: multidict>=4.0 in c:\\users\\ariel\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from yarl<2.0,>=1.6->gql[requests]->cryoet_data_portal) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\ariel\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from yarl<2.0,>=1.6->gql[requests]->cryoet_data_portal) (0.2.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ariel\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.36.0,>=1.35.0->boto3->cryoet_data_portal) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install cryoet_data_portal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "os.chdir(\"./synthetic_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\ariel\\\\Downloads\\\\czii-cryo-et-object-identification\\\\synthetic_data'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 567M/36.7G [00:54<58:10, 10.3MiB/s]   \n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '1:0.5c8FD169'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m client \u001b[38;5;241m=\u001b[39m Client()\n\u001b[0;32m      5\u001b[0m dataset \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mget_by_id(client, \u001b[38;5;241m10441\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_everything\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ariel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\cryoet_data_portal\\_models.py:423\u001b[0m, in \u001b[0;36mDataset.download_everything\u001b[1;34m(self, dest_path)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Download all of the data for this dataset.\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \n\u001b[0;32m    419\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;124;03m    dest_path (Optional[str], optional): Choose a destination directory. Defaults to $CWD.\u001b[39;00m\n\u001b[0;32m    421\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    422\u001b[0m recursive_prefix \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ms3_prefix\u001b[38;5;241m.\u001b[39mstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 423\u001b[0m \u001b[43mdownload_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ms3_prefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecursive_prefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdest_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ariel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\cryoet_data_portal\\_file_tools.py:137\u001b[0m, in \u001b[0;36mdownload_directory\u001b[1;34m(s3_url, recursive_from_prefix, dest_path, with_progress)\u001b[0m\n\u001b[0;32m    135\u001b[0m         progress_bar\u001b[38;5;241m.\u001b[39mupdate(size)\n\u001b[0;32m    136\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m \u001b[43ms3_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbucket\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mCallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ariel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\boto3\\s3\\inject.py:192\u001b[0m, in \u001b[0;36mdownload_file\u001b[1;34m(self, Bucket, Key, Filename, ExtraArgs, Callback, Config)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Download an S3 object to a file.\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \n\u001b[0;32m    159\u001b[0m \u001b[38;5;124;03mUsage::\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;124;03m    transfer.\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m S3Transfer(\u001b[38;5;28mself\u001b[39m, Config) \u001b[38;5;28;01mas\u001b[39;00m transfer:\n\u001b[1;32m--> 192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtransfer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbucket\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBucket\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mKey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mExtraArgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ariel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\boto3\\s3\\transfer.py:406\u001b[0m, in \u001b[0;36mS3Transfer.download_file\u001b[1;34m(self, bucket, key, filename, extra_args, callback)\u001b[0m\n\u001b[0;32m    402\u001b[0m future \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_manager\u001b[38;5;241m.\u001b[39mdownload(\n\u001b[0;32m    403\u001b[0m     bucket, key, filename, extra_args, subscribers\n\u001b[0;32m    404\u001b[0m )\n\u001b[0;32m    405\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 406\u001b[0m     \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[38;5;66;03m# This is for backwards compatibility where when retries are\u001b[39;00m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;66;03m# exceeded we need to throw the same error from boto3 instead of\u001b[39;00m\n\u001b[0;32m    409\u001b[0m \u001b[38;5;66;03m# s3transfer's built in RetriesExceededError as current users are\u001b[39;00m\n\u001b[0;32m    410\u001b[0m \u001b[38;5;66;03m# catching the boto3 one instead of the s3transfer exception to do\u001b[39;00m\n\u001b[0;32m    411\u001b[0m \u001b[38;5;66;03m# their own retries.\u001b[39;00m\n\u001b[0;32m    412\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m S3TransferRetriesExceededError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\ariel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\s3transfer\\futures.py:103\u001b[0m, in \u001b[0;36mTransferFuture.result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresult\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    100\u001b[0m         \u001b[38;5;66;03m# Usually the result() method blocks until the transfer is done,\u001b[39;00m\n\u001b[0;32m    101\u001b[0m         \u001b[38;5;66;03m# however if a KeyboardInterrupt is raised we want want to exit\u001b[39;00m\n\u001b[0;32m    102\u001b[0m         \u001b[38;5;66;03m# out of this and propagate the exception.\u001b[39;00m\n\u001b[1;32m--> 103\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_coordinator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    105\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel()\n",
      "File \u001b[1;32mc:\\Users\\ariel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\s3transfer\\futures.py:264\u001b[0m, in \u001b[0;36mTransferCoordinator.result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;66;03m# Once done waiting, raise an exception if present or return the\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;66;03m# final result.\u001b[39;00m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m--> 264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[1;32mc:\\Users\\ariel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\s3transfer\\tasks.py:135\u001b[0m, in \u001b[0;36mTask.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;66;03m# If the task is not done (really only if some other related\u001b[39;00m\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;66;03m# task to the TransferFuture had failed) then execute the task's\u001b[39;00m\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;66;03m# main() method.\u001b[39;00m\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_coordinator\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m--> 135\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_main\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_and_set_exception(e)\n",
      "File \u001b[1;32mc:\\Users\\ariel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\s3transfer\\tasks.py:158\u001b[0m, in \u001b[0;36mTask._execute_main\u001b[1;34m(self, kwargs)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;66;03m# Log what is about to be executed.\u001b[39;00m\n\u001b[0;32m    156\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecuting task \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with kwargs \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs_to_display\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 158\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_main\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;66;03m# If the task is the final task, then set the TransferFuture's\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;66;03m# value to the return value from main().\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_final:\n",
      "File \u001b[1;32mc:\\Users\\ariel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\s3transfer\\download.py:640\u001b[0m, in \u001b[0;36mIOWriteTask._main\u001b[1;34m(self, fileobj, data, offset)\u001b[0m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_main\u001b[39m(\u001b[38;5;28mself\u001b[39m, fileobj, data, offset):\n\u001b[0;32m    634\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Pulls off an io queue to write contents to a file\u001b[39;00m\n\u001b[0;32m    635\u001b[0m \n\u001b[0;32m    636\u001b[0m \u001b[38;5;124;03m    :param fileobj: The file handle to write content to\u001b[39;00m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;124;03m    :param data: The data to write\u001b[39;00m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;124;03m    :param offset: The offset to write the data to.\u001b[39;00m\n\u001b[0;32m    639\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 640\u001b[0m     \u001b[43mfileobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseek\u001b[49m\u001b[43m(\u001b[49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    641\u001b[0m     fileobj\u001b[38;5;241m.\u001b[39mwrite(data)\n",
      "File \u001b[1;32mc:\\Users\\ariel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\s3transfer\\utils.py:385\u001b[0m, in \u001b[0;36mDeferredOpenFile.seek\u001b[1;34m(self, where, whence)\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mseek\u001b[39m(\u001b[38;5;28mself\u001b[39m, where, whence\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m--> 385\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open_if_needed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fileobj\u001b[38;5;241m.\u001b[39mseek(where, whence)\n",
      "File \u001b[1;32mc:\\Users\\ariel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\s3transfer\\utils.py:368\u001b[0m, in \u001b[0;36mDeferredOpenFile._open_if_needed\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_if_needed\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    367\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fileobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 368\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fileobj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    369\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_byte \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    370\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fileobj\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_byte)\n",
      "File \u001b[1;32mc:\\Users\\ariel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\s3transfer\\utils.py:279\u001b[0m, in \u001b[0;36mOSUtils.open\u001b[1;34m(self, filename, mode)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename, mode):\n\u001b[1;32m--> 279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, mode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '1:0.5c8FD169'"
     ]
    }
   ],
   "source": [
    "from cryoet_data_portal import Client, Dataset\n",
    "\n",
    "client = Client()\n",
    "\n",
    "dataset = Dataset.get_by_id(client, 10441)\n",
    "dataset.download_everything(\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------LOADING TOMOGRAM DATA AND PARTICLE COORDINATES-----------------#\n",
    "\n",
    "# Define the experiment runs to load\n",
    "experiment_runs = [\"TS_5_4\", \"TS_69_2\", \"TS_6_4\", \"TS_6_6\", \"TS_73_6\", \"TS_86_3\", \"TS_99_9\"]\n",
    "particle_types = {\"virus-like-particle\":1, \"apo-ferritin\":2, \"beta-amylase\":3, \"beta-galactosidase\":4, \"ribosome\":5, \"thyroglobulin\":6}\n",
    "voxel_spacing = [10.0, 10.0, 10.0]  # 10 angstroms per voxel\n",
    "\n",
    "# Initialize lists to store combined data\n",
    "combined_tomogram_data = []\n",
    "combined_particle_coords = {pt: [] for pt in particle_types}\n",
    "\n",
    "# Track the cumulative z-depth for coordinate translation\n",
    "cumulative_z_depth = 0\n",
    "\n",
    "# Load and combine data from all experiment runs\n",
    "for experiment_run in experiment_runs:\n",
    "    zarr_file_path = os.path.join(\"train\", \"static\", \"ExperimentRuns\", experiment_run, \"VoxelSpacing10.000\", \"denoised.zarr\")\n",
    "    json_base_path = os.path.join(\"train\", \"overlay\", \"ExperimentRuns\", experiment_run, \"Picks\")\n",
    "\n",
    "    # Load the Zarr file\n",
    "    try:\n",
    "        tomogram = zarr.open(zarr_file_path, mode=\"r\")\n",
    "        tomogram_data = tomogram[\"0\"][:]  # Load into memory as a NumPy array\n",
    "        print(f\"Tomogram shape for {experiment_run} (z, y, x):\", tomogram_data.shape)\n",
    "        tomogram_data = (tomogram_data - tomogram_data.mean()) / tomogram_data.std()\n",
    "        combined_tomogram_data.append(tomogram_data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading Zarr file for {experiment_run}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Load and transform particle coordinates for all types\n",
    "    for particle_type in particle_types:\n",
    "        json_file_path = os.path.join(json_base_path, f\"{particle_type}.json\")\n",
    "        try:\n",
    "            with open(json_file_path, \"r\") as file:\n",
    "                data = json.load(file)\n",
    "            points = data[\"points\"]\n",
    "\n",
    "            # Convert from real-world coordinates (angstroms) to voxel indices and reorder to (z, y, x)\n",
    "            coords = np.array([\n",
    "                [\n",
    "                    (p[\"location\"][\"z\"] / voxel_spacing[0]) + cumulative_z_depth,  # Translate z-coordinate\n",
    "                     p[\"location\"][\"y\"] / voxel_spacing[1],  # y-coordinate\n",
    "                     p[\"location\"][\"x\"] / voxel_spacing[2],  # x-coordinate\n",
    "                ]\n",
    "                for p in points\n",
    "            ])\n",
    "            combined_particle_coords[particle_type].extend(coords)\n",
    "            print(f\"Loaded {len(coords)} points for {particle_type} in {experiment_run}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading JSON file for {particle_type} in {experiment_run}: {e}\")\n",
    "\n",
    "    # Update cumulative_z_depth for the next tomogram\n",
    "    cumulative_z_depth += tomogram_data.shape[0]\n",
    "\n",
    "# Combine all tomogram data into a single array\n",
    "combined_tomogram_data = np.concatenate(combined_tomogram_data, axis=0)\n",
    "print(\"Combined tomogram shape (z, y, x):\", combined_tomogram_data.shape)\n",
    "\n",
    "# Print total number of particles\n",
    "total_particles = sum(len(coords) for coords in combined_particle_coords.values())\n",
    "print(f\"Total number of particles: {total_particles}\")\n",
    "\n",
    "# --------------------------------------------------------------------------------------------#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------Combine tomograms and sample cubes with particles in it-----------------#\n",
    "\n",
    "# Dimensions of the combined tomogram data\n",
    "data_shape = combined_tomogram_data.shape\n",
    "cube_size = (96, 96, 96)\n",
    "particle_label_size = (8, 8, 8)\n",
    "background_id = 0\n",
    "\n",
    "# Calculate the number of cubes in each dimension\n",
    "num_cubes_z = data_shape[0] // cube_size[0]\n",
    "num_cubes_y = data_shape[1] // cube_size[1]\n",
    "num_cubes_x = data_shape[2] // cube_size[2]\n",
    "\n",
    "# Create a list of all possible cube indices\n",
    "cubes = []\n",
    "particle_cubes = []\n",
    "non_particle_cubes = []\n",
    "\n",
    "for z in range(num_cubes_z):\n",
    "    for y in range(num_cubes_y):\n",
    "        for x in range(num_cubes_x):\n",
    "            cubes.append((z, y, x))\n",
    "\n",
    "# Separate cubes into particle-containing and non-particle cubes\n",
    "def contains_particle(cube_start, particle_coords):\n",
    "    for coords in particle_coords.values():\n",
    "        for coord in coords:\n",
    "            z, y, x = coord.astype(int)\n",
    "            if (\n",
    "                cube_start[0] <= z < cube_start[0] + cube_size[0] and\n",
    "                cube_start[1] <= y < cube_start[1] + cube_size[1] and\n",
    "                cube_start[2] <= x < cube_start[2] + cube_size[2]\n",
    "            ):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "for cz, cy, cx in cubes:\n",
    "    cube_start = (cz * cube_size[0], cy * cube_size[1], cx * cube_size[2])\n",
    "    if contains_particle(cube_start, combined_particle_coords):\n",
    "        particle_cubes.append((cz, cy, cx))\n",
    "    else:\n",
    "        non_particle_cubes.append((cz, cy, cx))\n",
    "\n",
    "# Limit non-particle cubes to 20% of the dataset\n",
    "num_non_particle_cubes = int(len(particle_cubes) * 0.1)\n",
    "selected_non_particle_cubes = random.sample(non_particle_cubes, num_non_particle_cubes)\n",
    "selected_cubes = particle_cubes + selected_non_particle_cubes\n",
    "print(f\"Selected {len(selected_cubes)} cubes for the dataset. Where {len(particle_cubes)} contain particles and {len(selected_non_particle_cubes)} do not.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create labels for a cube\n",
    "def create_labels(cube_start, cube_data, particle_coords):\n",
    "    labels = np.full(cube_data.shape, background_id, dtype=int)\n",
    "    \n",
    "    for particle_type, coords in particle_coords.items():\n",
    "        for coord in coords:\n",
    "            z, y, x = coord.astype(int)\n",
    "            z_rel, y_rel, x_rel = z - cube_start[0], y - cube_start[1], x - cube_start[2]\n",
    "            \n",
    "            if (\n",
    "                0 <= z_rel < cube_data.shape[0] and\n",
    "                0 <= y_rel < cube_data.shape[1] and\n",
    "                0 <= x_rel < cube_data.shape[2]\n",
    "            ):\n",
    "                z_start = max(0, z_rel - particle_label_size[0] // 2)\n",
    "                z_end = min(cube_data.shape[0], z_rel + particle_label_size[0] // 2 + 1)\n",
    "                y_start = max(0, y_rel - particle_label_size[1] // 2)\n",
    "                y_end = min(cube_data.shape[1], y_rel + particle_label_size[1] // 2 + 1)\n",
    "                x_start = max(0, x_rel - particle_label_size[2] // 2)\n",
    "                x_end = min(cube_data.shape[2], x_rel + particle_label_size[2] // 2 + 1)\n",
    "\n",
    "                labels[z_start:z_end, y_start:y_end, x_start:x_end] = particle_types[particle_type]  # Unique ID for particle type\n",
    "    \n",
    "    return labels\n",
    "\n",
    "# Define a PyTorch dataset class\n",
    "class TomogramDataset(Dataset):\n",
    "    def __init__(self, tomogram_data, selected_cubes, particle_coords):\n",
    "        self.tomogram_data = tomogram_data\n",
    "        self.selected_cubes = selected_cubes\n",
    "        self.particle_coords = particle_coords\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.selected_cubes)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cz, cy, cx = self.selected_cubes[idx]\n",
    "        z_start, z_end = cz * cube_size[0], (cz + 1) * cube_size[0]\n",
    "        y_start, y_end = cy * cube_size[1], (cy + 1) * cube_size[1]\n",
    "        x_start, x_end = cx * cube_size[2], (cx + 1) * cube_size[2]\n",
    "\n",
    "        cube_data = self.tomogram_data[z_start:z_end, y_start:y_end, x_start:x_end]\n",
    "        cube_start = (z_start, y_start, x_start)\n",
    "        labels = create_labels(cube_start, cube_data, self.particle_coords)\n",
    "\n",
    "        return torch.tensor(cube_data, dtype=torch.float32), torch.tensor(labels, dtype=torch.int64)\n",
    "\n",
    "# Create the dataset\n",
    "particle_dataset = TomogramDataset(combined_tomogram_data, selected_cubes, combined_particle_coords)\n",
    "\n",
    "# ---------------------------------------------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- VISUALIZE Combined Tomogram Data ----------------------------#\n",
    "\n",
    "# Define a color map for label IDs\n",
    "label_colors = {\n",
    "    1: \"red\",        # virus-like-particle\n",
    "    2: \"green\",      # apo-ferritin\n",
    "    3: \"blue\",       # beta-amylase\n",
    "    4: \"yellow\",     # beta-galactosidase\n",
    "    5: \"magenta\",    # ribosome\n",
    "    6: \"cyan\",       # thyroglobulin\n",
    "}\n",
    "\n",
    "# Function to visualize the combined tomogram with particles in 3D using napari\n",
    "def visualize_combined_tomogram(tomogram_data, particle_coords):\n",
    "    # Create a napari viewer\n",
    "    viewer = napari.Viewer()\n",
    "\n",
    "    # Add the combined tomogram data as a 3D volume\n",
    "    viewer.add_image(tomogram_data, name=\"Combined Tomogram\")\n",
    "\n",
    "    # Collect all particle coordinates and their label IDs\n",
    "    all_particles = []\n",
    "    all_labels = []\n",
    "    for particle_type, coords in particle_coords.items():\n",
    "        label_id = particle_types[particle_type]\n",
    "        all_particles.extend(coords)\n",
    "        all_labels.extend([label_id] * len(coords))\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    all_particles = np.array(all_particles)\n",
    "    all_labels = np.array(all_labels)\n",
    "\n",
    "    # Assign colors to each particle based on its label ID\n",
    "    colors = [label_colors[label] for label in all_labels]\n",
    "\n",
    "    # Add the particles as a 3D points layer with different colors\n",
    "    if all_particles.size > 0:\n",
    "        viewer.add_points(\n",
    "            all_particles,\n",
    "            name=\"Particles\",\n",
    "            face_color=colors,\n",
    "            size=5,\n",
    "            opacity=0.8,\n",
    "        )\n",
    "\n",
    "    # Start the napari event loop\n",
    "    napari.run()\n",
    "\n",
    "# Visualize the combined tomogram with particles\n",
    "print(\"Visualizing the combined tomogram with particles...\")\n",
    "visualize_combined_tomogram(combined_tomogram_data, combined_particle_coords)\n",
    "# ---------------------------------------------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_selected_cubes(tomogram_data, particle_coords, selected_cubes, cube_size):\n",
    "    # Create a napari viewer\n",
    "    viewer = napari.Viewer()\n",
    "\n",
    "    # Iterate through the 10 selected cubes and visualize them\n",
    "    for idx, (cz, cy, cx) in enumerate(selected_cubes[:10]):  # Limit to 10 cubes\n",
    "        # Define cube boundaries\n",
    "        z_start, y_start, x_start = cz * cube_size[0], cy * cube_size[1], cx * cube_size[2]\n",
    "        z_end, y_end, x_end = z_start + cube_size[0], y_start + cube_size[1], x_start + cube_size[2]\n",
    "\n",
    "        # Extract cube data from the tomogram\n",
    "        cube_data = tomogram_data[z_start:z_end, y_start:y_end, x_start:x_end]\n",
    "\n",
    "        # Collect particle coordinates and labels within the cube\n",
    "        cube_particles = []\n",
    "        cube_labels = []\n",
    "        for particle_type, coords in particle_coords.items():\n",
    "            label_id = particle_types[particle_type]\n",
    "            for coord in coords:\n",
    "                z, y, x = coord.astype(int)\n",
    "                if z_start <= z < z_end and y_start <= y < y_end and x_start <= x < x_end:\n",
    "                    # Adjust coordinates to cube-local space\n",
    "                    cube_particles.append([z - z_start, y - y_start, x - x_start])\n",
    "                    cube_labels.append(label_id)\n",
    "\n",
    "        # Convert to numpy arrays\n",
    "        cube_particles = np.array(cube_particles)\n",
    "        cube_labels = np.array(cube_labels)\n",
    "\n",
    "        # Assign colors to each particle based on its label ID\n",
    "        colors = [label_colors[label] for label in cube_labels]\n",
    "\n",
    "        # Add the cube data as a volume\n",
    "        viewer.add_image(cube_data, name=f\"Cube {idx + 1}\", colormap=\"gray\")\n",
    "\n",
    "        # Add the particles as a points layer\n",
    "        if cube_particles.size > 0:\n",
    "            viewer.add_points(\n",
    "                cube_particles,\n",
    "                name=f\"Particles in Cube {idx + 1}\",\n",
    "                face_color=colors,\n",
    "                size=5,\n",
    "                opacity=0.8,\n",
    "            )\n",
    "\n",
    "    # Start the napari event loop\n",
    "    napari.run()\n",
    "\n",
    "# Visualize the first 10 selected cubes\n",
    "print(\"Visualizing 10 selected cubes with particles...\")\n",
    "visualize_selected_cubes(combined_tomogram_data, combined_particle_coords, selected_cubes, cube_size)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
