{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ariel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Library imports\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import sys\n",
    "import zarr\n",
    "import json\n",
    "import os\n",
    "import napari\n",
    "from rich import print as rprint  # Import rich's print function\n",
    "import copy\n",
    "from dataclasses import dataclass\n",
    "from monai.transforms import Compose, RandFlipd, RandRotated, RandGaussianNoised, RandAdjustContrastd\n",
    "# set torch and cuda seed for reproducibility\n",
    "torch.manual_seed(37)\n",
    "torch.cuda.manual_seed(37)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final combined tomogram shape: (5400, 630, 630)\n",
      "Final combined label shape: (5400, 630, 630)\n"
     ]
    }
   ],
   "source": [
    "# -------------LOADING TOMOGRAM DATA AND SEGMENTATION MASKS-----------------#\n",
    "\n",
    "experiment_runs = [\"TS_\" + str(i) for i in range(27)]\n",
    "particle_types = {\"virus-like-particle\": 1, \"apo-ferritin\": 2, \"beta-amylase\": 3, \n",
    "                  \"beta-galactosidase\": 4, \"ribosome\": 5, \"thyroglobulin\": 6}\n",
    "type_mapping = {\n",
    "    \"virus-like-particle\": [\"pp7_vlp\", 6], \"apo-ferritin\": [\"ferritin_complex\", 1], \n",
    "    \"beta-amylase\": [\"beta_amylase\", 2], \"beta-galactosidase\": [\"beta_galactosidase\", 3], \n",
    "    \"ribosome\": [\"cytosolic_ribosome\", 4], \"thyroglobulin\": [\"thyroglobulin\", 5]\n",
    "}\n",
    "\n",
    "# Initialize lists to store tomograms and labels\n",
    "combined_tomogram_data = []\n",
    "combined_label_data = []\n",
    "\n",
    "for experiment_run in experiment_runs:\n",
    "    zarr_file_path = os.path.join(\"aws_data\", \"10441\", experiment_run, \"Reconstructions\", \n",
    "                                  \"VoxelSpacing10.000\", \"Tomograms\", \"100\", experiment_run)\n",
    "\n",
    "    try:\n",
    "        # Load tomogram\n",
    "        tomogram = zarr.open(zarr_file_path + \".zarr\", mode=\"r\")\n",
    "        tomogram_data = tomogram[\"0\"][:]\n",
    "        # tomogram_data = (tomogram_data - tomogram_data.mean()) / tomogram_data.std()\n",
    "        \n",
    "        # Initialize label cube for this tomogram\n",
    "        label_cube = np.zeros_like(tomogram_data, dtype=np.uint8)\n",
    "\n",
    "        # Process segmentation masks\n",
    "        for particle_type, particle_id in particle_types.items():\n",
    "            name, number = type_mapping[particle_type]\n",
    "            segmentation_mask_path = os.path.join(\"aws_data\", \"10441\", experiment_run, \"Reconstructions\", \n",
    "                                                  \"VoxelSpacing10.000\", \"Annotations\", \n",
    "                                                  \"10\" + str(number), name + \"-1.0_segmentationmask.zarr\")\n",
    "            try:\n",
    "                segmentation_mask = zarr.open(segmentation_mask_path, mode=\"r\")[\"0\"][:]\n",
    "                label_cube[segmentation_mask > 0] = particle_id  # Assign particle ID where mask exists\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading segmentation mask for {particle_type} in {experiment_run}: {e}\")\n",
    "\n",
    "        # Append processed data\n",
    "        combined_tomogram_data.append(tomogram_data)\n",
    "        combined_label_data.append(label_cube)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading Zarr file for {experiment_run}: {e}\")\n",
    "\n",
    "# Concatenate all tomograms and labels along the Z-dimension\n",
    "combined_tomogram_data = np.concatenate(combined_tomogram_data, axis=0)\n",
    "combined_label_data = np.concatenate(combined_label_data, axis=0)\n",
    "\n",
    "print(\"Final combined tomogram shape:\", combined_tomogram_data.shape)\n",
    "print(\"Final combined label shape:\", combined_label_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ariel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\napari\\_vispy\\layers\\scalar_field.py:198: UserWarning: data shape (5400, 630, 630) exceeds GL_MAX_TEXTURE_SIZE 2048 in at least one axis and will be downsampled. Rendering is currently in 3D mode.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# -------------VISUALIZATION WITH NAPARI-----------------#\n",
    "\n",
    "def visualize_tomogram_and_labels(tomogram_data, label_data):\n",
    "    viewer = napari.Viewer()\n",
    "    viewer.add_image(\n",
    "        tomogram_data,\n",
    "        name=\"Tomogram Data\",\n",
    "        # contrast_limits=[np.min(tomogram_data), np.max(tomogram_data)],\n",
    "        colormap=\"gray\",\n",
    "    )\n",
    "    viewer.add_labels(\n",
    "        label_data,\n",
    "        name=\"Label Data\",\n",
    "        opacity=0.5,\n",
    "    )\n",
    "    napari.run()\n",
    "\n",
    "# Visualize the dataset\n",
    "visualize_tomogram_and_labels(combined_tomogram_data, combined_label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 2016 cubes for the dataset. Where 1965 contain particles and 51 do not.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -------------Combine tomograms and sample cubes with particles in it-----------------#\n",
    "label_cube = combined_label_data\n",
    "# Dimensions of the combined tomogram data\n",
    "data_shape = combined_tomogram_data.shape\n",
    "cube_size = (96, 96, 96)\n",
    "background_id = 0\n",
    "\n",
    "# Calculate the number of cubes in each dimension\n",
    "num_cubes_z = data_shape[0] // cube_size[0]\n",
    "num_cubes_y = data_shape[1] // cube_size[1]\n",
    "num_cubes_x = data_shape[2] // cube_size[2]\n",
    "\n",
    "# Create a list of all possible cube indices\n",
    "cubes = []\n",
    "particle_cubes = []\n",
    "non_particle_cubes = []\n",
    "\n",
    "for z in range(num_cubes_z):\n",
    "    for y in range(num_cubes_y):\n",
    "        for x in range(num_cubes_x):\n",
    "            cubes.append((z, y, x))\n",
    "\n",
    "# Separate cubes into particle-containing and non-particle cubes\n",
    "def contains_particle(cube_start, label_cube):\n",
    "    z_start, y_start, x_start = cube_start\n",
    "    z_end, y_end, x_end = z_start + cube_size[0], y_start + cube_size[1], x_start + cube_size[2]\n",
    "    return np.any(label_cube[z_start:z_end, y_start:y_end, x_start:x_end] > 0)\n",
    "\n",
    "for cz, cy, cx in cubes:\n",
    "    cube_start = (cz * cube_size[0], cy * cube_size[1], cx * cube_size[2])\n",
    "    if contains_particle(cube_start, label_cube):\n",
    "        particle_cubes.append((cz, cy, cx))\n",
    "    else:\n",
    "        non_particle_cubes.append((cz, cy, cx))\n",
    "\n",
    "# Limit non-particle cubes to 10% of the dataset\n",
    "num_non_particle_cubes = min(len(non_particle_cubes), int(len(particle_cubes) * 0.1))\n",
    "\n",
    "selected_non_particle_cubes = random.sample(non_particle_cubes, num_non_particle_cubes)\n",
    "selected_cubes = particle_cubes + selected_non_particle_cubes\n",
    "print(f\"Selected {len(selected_cubes)} cubes for the dataset. Where {len(particle_cubes)} contain particles and {len(selected_non_particle_cubes)} do not.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2016, 1, 96, 96, 96]) torch.Size([2016, 4096])\n"
     ]
    }
   ],
   "source": [
    "class DataCreator():\n",
    "    def __init__(self, tomogram_data, label_cube, selected_cubes):\n",
    "        self.tomogram_data = tomogram_data\n",
    "        self.label_cube = label_cube\n",
    "        self.selected_cubes = selected_cubes\n",
    "        self.cube_size = (96, 96, 96)\n",
    "        self.subcube_size = (6, 6, 6)\n",
    "        self.background_id = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.selected_cubes)\n",
    "\n",
    "    def getitem(self, idx):\n",
    "        cz, cy, cx = self.selected_cubes[idx]\n",
    "        z_start, z_end = cz * self.cube_size[0], (cz + 1) * self.cube_size[0]\n",
    "        y_start, y_end = cy * self.cube_size[1], (cy + 1) * self.cube_size[1]\n",
    "        x_start, x_end = cx * self.cube_size[2], (cx + 1) * self.cube_size[2]\n",
    "\n",
    "        cube_data = self.tomogram_data[z_start:z_end, y_start:y_end, x_start:x_end]\n",
    "        labels = self.generate_labels(z_start, y_start, x_start)\n",
    "\n",
    "        cube_data = np.expand_dims(cube_data, axis=0)  # Add channel dimension\n",
    "        return (\n",
    "            torch.tensor(cube_data, dtype=torch.float32),\n",
    "            torch.tensor(labels, dtype=torch.int64),\n",
    "        )\n",
    "\n",
    "    def generate_labels(self, z_start, y_start, x_start):\n",
    "        mini_cube_labels = []\n",
    "        \n",
    "        for z in range(0, self.cube_size[2], self.subcube_size[2]):  # Width first\n",
    "            for y in range(0, self.cube_size[1], self.subcube_size[1]):  # Height second\n",
    "                for x in range(0, self.cube_size[0], self.subcube_size[0]):  # Depth last\n",
    "                    mini_cube = self.label_cube[\n",
    "                        z_start + z:z_start + z + self.subcube_size[0],\n",
    "                        y_start + y:y_start + y + self.subcube_size[1],\n",
    "                        x_start + x:x_start + x + self.subcube_size[2],\n",
    "                    ]\n",
    "                    unique, counts = np.unique(mini_cube, return_counts=True)\n",
    "                    label_coverage = dict(zip(unique, counts))\n",
    "                    total_voxels = np.prod(self.subcube_size)\n",
    "\n",
    "                    dominant_label = self.background_id\n",
    "                    max_coverage = 0\n",
    "\n",
    "                    for label, coverage in label_coverage.items():\n",
    "                        if label != self.background_id and coverage / total_voxels >= 0.37 and coverage > max_coverage:\n",
    "                            dominant_label = label\n",
    "                            max_coverage = coverage\n",
    "\n",
    "                    mini_cube_labels.append(dominant_label)\n",
    "\n",
    "        return np.array(mini_cube_labels)\n",
    "\n",
    "    def generate_data(self):\n",
    "        tomogram_data = torch.zeros((len(self.selected_cubes), 1, *self.cube_size))\n",
    "        segmentation_labels = torch.zeros((len(self.selected_cubes), int(self.cube_size[0]**3/self.subcube_size[0]**3)), dtype=torch.int64)\n",
    "\n",
    "        for idx in range(len(self.selected_cubes)):\n",
    "            data_tensor, label_tensor= self.getitem(idx)\n",
    "            tomogram_data[idx] = data_tensor\n",
    "            segmentation_labels[idx] = label_tensor\n",
    "        return tomogram_data, segmentation_labels\n",
    "\n",
    "# Data Creator\n",
    "data_creator = DataCreator(\n",
    "    combined_tomogram_data, label_cube, selected_cubes\n",
    ")\n",
    "\n",
    "input_data, segmentation_labels = data_creator.generate_data()\n",
    "\n",
    "print(input_data.shape, segmentation_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'combined_tomogram_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[43mcombined_tomogram_data\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m label_cube\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m combined_label_data\n",
      "\u001b[1;31mNameError\u001b[0m: name 'combined_tomogram_data' is not defined"
     ]
    }
   ],
   "source": [
    "del combined_tomogram_data\n",
    "del label_cube\n",
    "del combined_label_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(particle_dataset_mini_cubes): 2016\n",
      "Cube Data Shape: torch.Size([1, 96, 96, 96])\n",
      "Labels Shape: torch.Size([4096])\n"
     ]
    }
   ],
   "source": [
    "# -------------DATASET IMPLEMENTATION-----------------#\n",
    "class TomogramDatasetMiniCubes(Dataset):\n",
    "    def __init__(self, tomogram_data, segmentation_labels):\n",
    "        assert tomogram_data.size(0) == segmentation_labels.size(0), f\"{tomogram_data.size(0)}, {segmentation_labels.size(0)}\"\n",
    "        self.tomogram_data = tomogram_data\n",
    "        self.segmentation_labels = segmentation_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.tomogram_data.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            self.tomogram_data[idx],\n",
    "            self.segmentation_labels[idx],\n",
    "        )\n",
    "\n",
    "\n",
    "particle_types = {\"virus-like-particle\": 1, \"apo-ferritin\": 2, \"beta-amylase\": 3, \n",
    "                  \"beta-galactosidase\": 4, \"ribosome\": 5, \"thyroglobulin\": 6}\n",
    "input_data, segmentation_labels = torch.load(\"segmentation_input_data.pt\"), torch.load(\"segmentation_labels.pt\")\n",
    "# Create the dataset\n",
    "particle_dataset_mini_cubes = TomogramDatasetMiniCubes(input_data, segmentation_labels)\n",
    "\n",
    "# Test the dataset\n",
    "cube_data, segmentation_labels = particle_dataset_mini_cubes[200]\n",
    "print(f\"len(particle_dataset_mini_cubes): {len(particle_dataset_mini_cubes)}\")\n",
    "print(\"Cube Data Shape:\", cube_data.shape)  # Should be (1, 96, 96, 96)\n",
    "print(\"Labels Shape:\", segmentation_labels.shape)        # Should be (4096,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first cube and its labels\n",
    "cube_data, labels = particle_dataset_mini_cubes[550]\n",
    "\n",
    "# Convert cube data to numpy (removing the channel dimension)\n",
    "sample_data = cube_data.squeeze().numpy()\n",
    "\n",
    "# Reshape labels from (16, 16, 16) to match subcube structure\n",
    "labels = labels.numpy().reshape((16, 16, 16))\n",
    "\n",
    "# Create an empty array for the upscaled labels\n",
    "scaled_labels = np.zeros_like(sample_data, dtype=np.int64)\n",
    "\n",
    "# Iterate over the (16, 16, 16) labels and assign them to (6, 6, 6) regions\n",
    "for z in range(16):\n",
    "    for y in range(16):\n",
    "        for x in range(16):\n",
    "            scaled_labels[z*6:(z+1)*6, y*6:(y+1)*6, x*6:(x+1)*6] = labels[z, y, x]\n",
    "\n",
    "# Create a napari viewer\n",
    "viewer = napari.Viewer()\n",
    "\n",
    "# Add the cube data as a 3D volume\n",
    "viewer.add_image(\n",
    "    sample_data, name='Tomogram Cube', colormap=\"gray\",\n",
    "    contrast_limits=[np.min(sample_data), np.max(sample_data)]\n",
    ")\n",
    "\n",
    "# Add the upsampled labels\n",
    "viewer.add_labels(scaled_labels, name='Scaled Mini-Cube Labels')\n",
    "\n",
    "# Start the napari event loop\n",
    "napari.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
    "        # regularization\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        # nh is \"number of heads\", hs is \"head size\", and C (number of channels) = nh * hs\n",
    "        # e.g. in GPT-2 (124M), n_head=11, hs=64, so nh*hs=C=768 channels in the Transformer\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal=False) # flash attention\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "        # output projection\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.gelu    = nn.GELU()\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = SelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_layers, decoder_layer):\n",
    "        super().__init__()\n",
    "        self.layers = clones(decoder_layer, num_layers)\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    def forward(self, tgt):\n",
    "        output = tgt\n",
    "\n",
    "        for mod in self.layers:\n",
    "            output = mod(output)\n",
    "        return output\n",
    "    \n",
    "class LinearTokenizer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.tokenizer = nn.Conv3d(\n",
    "            in_channels=1,  # Input channels (C=1)\n",
    "            out_channels=config.n_embd,\n",
    "            kernel_size=config.token_width,\n",
    "            stride=config.token_width,\n",
    "            padding=0,\n",
    "            bias=True  # Optional\n",
    "        )\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (B, 1, D=96, H=96, W=96)\n",
    "        x = self.tokenizer(x)  # Output shape: (B, n_embd, D_blocks=8, H_blocks=8, W_blocks=8)\n",
    "        \n",
    "        # Efficient reshaping and transposing\n",
    "        x = x.reshape(x.size(0), x.size(1), -1).transpose(-2, -1)  # (B, 512, n_embd)\n",
    "        \n",
    "        return x\n",
    "\n",
    "@dataclass\n",
    "class LinearConfig:\n",
    "    block_size: int = int((96**3)/(6**3)) # max sequence length\n",
    "    token_width: int = 6 # width of the cube\n",
    "    n_layer: int = 4 # number of layers\n",
    "    n_head: int = 16 # number of heads\n",
    "    n_embd: int = 144 # embedding dimension\n",
    "\n",
    "\n",
    "class BaseModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.tokenizer = LinearTokenizer(config)\n",
    "        self.positional_embedding = nn.Parameter(torch.zeros(config.block_size, config.n_embd))\n",
    "        self.transformer = Transformer(config.n_layer, TransformerBlock(config))\n",
    "        self.decoder = nn.Linear(config.n_embd, len(particle_types) + 1)  # Output layer\n",
    "\n",
    "    def ff(self, x):\n",
    "        x = self.tokenizer(x) # (N, 1, 96, 96, 96) -> (N, 8 * 8 * 8, n_embd)\n",
    "        x = x + self.positional_embedding # (N, n_embd, 8, 8, 8) -> (N, n_embd, 512) -> (N, 512, n_embd)\n",
    "        x = self.transformer(x) # (N, 512, n_embd) -> (N, 512, n_embd)\n",
    "        x = self.decoder(x) # (N, 512, n_embd) -> (N, 512, 7)        \n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):   \n",
    "        return self.ff(x)\n",
    "    \n",
    "\n",
    "class ContrastiveModel(BaseModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        # Summarizer token\n",
    "        self.summarizer = nn.Parameter(torch.zeros(1, 1, config.n_embd))\n",
    "        self.projection_head = nn.Sequential(\n",
    "            nn.Linear(config.n_embd, config.n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(config.n_embd, 128)  # Project to lower-dimensional space\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tokenizer(x) # (N, 1, 96, 96, 96) -> (N, 8 * 8 * 8, n_embd)\n",
    "        x = x + self.positional_embedding # (N, n_embd, 8, 8, 8) -> (N, n_embd, n_tokens) -> (N, n_tokens, n_embd)\n",
    "        \n",
    "        # Append Summarizer token\n",
    "        x = torch.cat([self.summarizer.repeat(x.size(0), 1, 1), x], dim=1)\n",
    "        \n",
    "        # Build representation using transformer\n",
    "        x = self.transformer(x) # (N, n_tokens + 1, n_embd) -> (N, n_tokens + 1, n_embd)\n",
    "        \n",
    "        # Extract the summarizer token\n",
    "        x = x[:, 0, :]  \n",
    "        # Project to contrastive space\n",
    "        return self.projection_head(x)\n",
    "    \n",
    "class SegmentationModel(ContrastiveModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "    \n",
    "    def ff_without_summarizer(self, x):\n",
    "        # ContrastiveModel inherits the ff (feedforward without summarizer) method from the BaseModel\n",
    "        return self.ff(x)\n",
    "    \n",
    "    def ff_with_summarizer(self, x):\n",
    "        x = self.tokenizer(x) # (N, 1, 96, 96, 96) -> (N, 8 * 8 * 8, n_embd)\n",
    "        x = x + self.positional_embedding # (N, n_embd, 8, 8, 8) -> (N, n_embd, n_tokens) -> (N, n_tokens, n_embd)\n",
    "        \n",
    "        # Append Summarizer token\n",
    "        x = torch.cat([self.summarizer.repeat(x.size(0), 1, 1), x], dim=1)\n",
    "        \n",
    "        # Build representation using transformer\n",
    "        x = self.transformer(x) # (N, n_tokens + 1, n_embd) -> (N, n_tokens + 1, n_embd)     \n",
    "\n",
    "        # Classify\n",
    "        x = self.decoder(x[:, 1:, :])\n",
    "        return x       \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ff_without_summarizer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "config = LinearConfig()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "segmentation_model = SegmentationModel(config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 1.66e+06\n",
      "SegmentationModel(\n",
      "  (tokenizer): LinearTokenizer(\n",
      "    (tokenizer): Conv3d(1, 144, kernel_size=(6, 6, 6), stride=(6, 6, 6))\n",
      "  )\n",
      "  (transformer): Transformer(\n",
      "    (layers): ModuleList(\n",
      "      (0-3): 4 x TransformerBlock(\n",
      "        (ln_1): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): SelfAttention(\n",
      "          (c_attn): Linear(in_features=144, out_features=432, bias=True)\n",
      "          (c_proj): Linear(in_features=144, out_features=144, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=144, out_features=576, bias=True)\n",
      "          (gelu): GELU(approximate='none')\n",
      "          (c_proj): Linear(in_features=576, out_features=144, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): Linear(in_features=144, out_features=7, bias=True)\n",
      "  (projection_head): Sequential(\n",
      "    (0): Linear(in_features=144, out_features=144, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=144, out_features=128, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch [1/5] Train Loss: 0.10550869, Validation Loss: 0.09365920\n",
      "...saved model at Epoch 1\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 50\u001b[0m\n\u001b[0;32m     48\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     49\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 50\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;66;03m# torch.cuda.synchronize()\u001b[39;00m\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;66;03m# et = time.time()\u001b[39;00m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;66;03m# print(f\"{(inputs.shape[0] * labels.shape[-1]) * 1000 / (et - st):.2f} tokens/ms\")\u001b[39;00m\n\u001b[0;32m     57\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "try:\n",
    "    # Load the trained model\n",
    "    segmentation_model.load_state_dict(torch.load(\"segmentation_model_mini_cubes.pth\"))\n",
    "    print(\"Loaded pretrained model\")\n",
    "except:\n",
    "    pass\n",
    "# print the # of parameters\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in segmentation_model.parameters() if p.requires_grad):.2e}\")\n",
    "print(segmentation_model)\n",
    "# Write train and validation dataloader for segmentation using particle_dataset_mini_cubes\n",
    "train_size = int(0.8 * len(particle_dataset_mini_cubes))\n",
    "val_size = len(particle_dataset_mini_cubes) - train_size\n",
    "train_dataset, val_dataset = random_split(particle_dataset_mini_cubes, [train_size, val_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "# Define optimizer, loss, and scheduler\n",
    "optimizer = optim.AdamW(segmentation_model.parameters(), lr=1e-3)\n",
    "# weights = torch.tensor([0.5] + [1 for _ in range(1, len(particle_types) + 1)]).to(device)  # Lower weight for background\n",
    "\n",
    "\"\"\"\n",
    "particle_types = {\"virus-like-particle\": 1, \"apo-ferritin\": 2, \"beta-amylase\": 3, \n",
    "                  \"beta-galactosidase\": 4, \"ribosome\": 5, \"thyroglobulin\": 6}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# background, virus-like particle, \"apo-ferritin\", \"beta-amylase\",\"beta-galactosidase\",\"ribosome\",\"thyroglobulin\"\n",
    "weights = torch.tensor([0.08, 0.16, 0.16, 0.16, 0.66, 0.16, 0.66]).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "# Training loop\n",
    "epochs = 5\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    segmentation_model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "        # Measure time to load batch\n",
    "        # st = time.time()\n",
    "        # torch.cuda.synchronize()\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = segmentation_model(inputs)\n",
    "        loss = criterion(outputs.view(-1, len(particle_types) + 1), labels.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        # torch.cuda.synchronize()\n",
    "        # et = time.time()\n",
    "        # print(f\"{(inputs.shape[0] * labels.shape[-1]) * 1000 / (et - st):.2f} tokens/ms\")\n",
    "\n",
    "\n",
    "    \n",
    "    val_loss = 0.0\n",
    "    segmentation_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = segmentation_model(inputs)\n",
    "            loss = criterion(outputs.view(-1, len(particle_types) + 1), labels.view(-1))\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    # Log epoch performance\n",
    "    print(f\"Epoch [{epoch + 1}/{epochs}] Train Loss: {train_loss / len(train_loader):.8f}, Validation Loss: {val_loss / len(val_loader):.8f}\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(segmentation_model.state_dict(), \"segmentation_model_mini_cubes.pth\")\n",
    "        print(f\"...saved model at Epoch {epoch + 1}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SegmentationModel(\n",
      "  (tokenizer): LinearTokenizer(\n",
      "    (tokenizer): Conv3d(1, 128, kernel_size=(6, 6, 6), stride=(6, 6, 6))\n",
      "  )\n",
      "  (transformer): Transformer(\n",
      "    (layers): ModuleList(\n",
      "      (0-9): 10 x TransformerBlock(\n",
      "        (ln_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): SelfAttention(\n",
      "          (c_attn): Linear(in_features=128, out_features=384, bias=True)\n",
      "          (c_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (gelu): GELU(approximate='none')\n",
      "          (c_proj): Linear(in_features=512, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): Linear(in_features=128, out_features=7, bias=True)\n",
      "  (projection_head): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for SegmentationModel:\n\tMissing key(s) in state_dict: \"positional_embedding\", \"summarizer\", \"tokenizer.tokenizer.weight\", \"tokenizer.tokenizer.bias\", \"transformer.layers.0.ln_1.weight\", \"transformer.layers.0.ln_1.bias\", \"transformer.layers.0.attn.c_attn.weight\", \"transformer.layers.0.attn.c_attn.bias\", \"transformer.layers.0.attn.c_proj.weight\", \"transformer.layers.0.attn.c_proj.bias\", \"transformer.layers.0.ln_2.weight\", \"transformer.layers.0.ln_2.bias\", \"transformer.layers.0.mlp.c_fc.weight\", \"transformer.layers.0.mlp.c_fc.bias\", \"transformer.layers.0.mlp.c_proj.weight\", \"transformer.layers.0.mlp.c_proj.bias\", \"transformer.layers.1.ln_1.weight\", \"transformer.layers.1.ln_1.bias\", \"transformer.layers.1.attn.c_attn.weight\", \"transformer.layers.1.attn.c_attn.bias\", \"transformer.layers.1.attn.c_proj.weight\", \"transformer.layers.1.attn.c_proj.bias\", \"transformer.layers.1.ln_2.weight\", \"transformer.layers.1.ln_2.bias\", \"transformer.layers.1.mlp.c_fc.weight\", \"transformer.layers.1.mlp.c_fc.bias\", \"transformer.layers.1.mlp.c_proj.weight\", \"transformer.layers.1.mlp.c_proj.bias\", \"transformer.layers.2.ln_1.weight\", \"transformer.layers.2.ln_1.bias\", \"transformer.layers.2.attn.c_attn.weight\", \"transformer.layers.2.attn.c_attn.bias\", \"transformer.layers.2.attn.c_proj.weight\", \"transformer.layers.2.attn.c_proj.bias\", \"transformer.layers.2.ln_2.weight\", \"transformer.layers.2.ln_2.bias\", \"transformer.layers.2.mlp.c_fc.weight\", \"transformer.layers.2.mlp.c_fc.bias\", \"transformer.layers.2.mlp.c_proj.weight\", \"transformer.layers.2.mlp.c_proj.bias\", \"transformer.layers.3.ln_1.weight\", \"transformer.layers.3.ln_1.bias\", \"transformer.layers.3.attn.c_attn.weight\", \"transformer.layers.3.attn.c_attn.bias\", \"transformer.layers.3.attn.c_proj.weight\", \"transformer.layers.3.attn.c_proj.bias\", \"transformer.layers.3.ln_2.weight\", \"transformer.layers.3.ln_2.bias\", \"transformer.layers.3.mlp.c_fc.weight\", \"transformer.layers.3.mlp.c_fc.bias\", \"transformer.layers.3.mlp.c_proj.weight\", \"transformer.layers.3.mlp.c_proj.bias\", \"transformer.layers.4.ln_1.weight\", \"transformer.layers.4.ln_1.bias\", \"transformer.layers.4.attn.c_attn.weight\", \"transformer.layers.4.attn.c_attn.bias\", \"transformer.layers.4.attn.c_proj.weight\", \"transformer.layers.4.attn.c_proj.bias\", \"transformer.layers.4.ln_2.weight\", \"transformer.layers.4.ln_2.bias\", \"transformer.layers.4.mlp.c_fc.weight\", \"transformer.layers.4.mlp.c_fc.bias\", \"transformer.layers.4.mlp.c_proj.weight\", \"transformer.layers.4.mlp.c_proj.bias\", \"transformer.layers.5.ln_1.weight\", \"transformer.layers.5.ln_1.bias\", \"transformer.layers.5.attn.c_attn.weight\", \"transformer.layers.5.attn.c_attn.bias\", \"transformer.layers.5.attn.c_proj.weight\", \"transformer.layers.5.attn.c_proj.bias\", \"transformer.layers.5.ln_2.weight\", \"transformer.layers.5.ln_2.bias\", \"transformer.layers.5.mlp.c_fc.weight\", \"transformer.layers.5.mlp.c_fc.bias\", \"transformer.layers.5.mlp.c_proj.weight\", \"transformer.layers.5.mlp.c_proj.bias\", \"transformer.layers.6.ln_1.weight\", \"transformer.layers.6.ln_1.bias\", \"transformer.layers.6.attn.c_attn.weight\", \"transformer.layers.6.attn.c_attn.bias\", \"transformer.layers.6.attn.c_proj.weight\", \"transformer.layers.6.attn.c_proj.bias\", \"transformer.layers.6.ln_2.weight\", \"transformer.layers.6.ln_2.bias\", \"transformer.layers.6.mlp.c_fc.weight\", \"transformer.layers.6.mlp.c_fc.bias\", \"transformer.layers.6.mlp.c_proj.weight\", \"transformer.layers.6.mlp.c_proj.bias\", \"transformer.layers.7.ln_1.weight\", \"transformer.layers.7.ln_1.bias\", \"transformer.layers.7.attn.c_attn.weight\", \"transformer.layers.7.attn.c_attn.bias\", \"transformer.layers.7.attn.c_proj.weight\", \"transformer.layers.7.attn.c_proj.bias\", \"transformer.layers.7.ln_2.weight\", \"transformer.layers.7.ln_2.bias\", \"transformer.layers.7.mlp.c_fc.weight\", \"transformer.layers.7.mlp.c_fc.bias\", \"transformer.layers.7.mlp.c_proj.weight\", \"transformer.layers.7.mlp.c_proj.bias\", \"transformer.layers.8.ln_1.weight\", \"transformer.layers.8.ln_1.bias\", \"transformer.layers.8.attn.c_attn.weight\", \"transformer.layers.8.attn.c_attn.bias\", \"transformer.layers.8.attn.c_proj.weight\", \"transformer.layers.8.attn.c_proj.bias\", \"transformer.layers.8.ln_2.weight\", \"transformer.layers.8.ln_2.bias\", \"transformer.layers.8.mlp.c_fc.weight\", \"transformer.layers.8.mlp.c_fc.bias\", \"transformer.layers.8.mlp.c_proj.weight\", \"transformer.layers.8.mlp.c_proj.bias\", \"transformer.layers.9.ln_1.weight\", \"transformer.layers.9.ln_1.bias\", \"transformer.layers.9.attn.c_attn.weight\", \"transformer.layers.9.attn.c_attn.bias\", \"transformer.layers.9.attn.c_proj.weight\", \"transformer.layers.9.attn.c_proj.bias\", \"transformer.layers.9.ln_2.weight\", \"transformer.layers.9.ln_2.bias\", \"transformer.layers.9.mlp.c_fc.weight\", \"transformer.layers.9.mlp.c_fc.bias\", \"transformer.layers.9.mlp.c_proj.weight\", \"transformer.layers.9.mlp.c_proj.bias\", \"decoder.weight\", \"decoder.bias\", \"projection_head.0.weight\", \"projection_head.0.bias\", \"projection_head.2.weight\", \"projection_head.2.bias\". \n\tUnexpected key(s) in state_dict: \"_orig_mod.positional_embedding\", \"_orig_mod.summarizer\", \"_orig_mod.tokenizer.tokenizer.weight\", \"_orig_mod.tokenizer.tokenizer.bias\", \"_orig_mod.transformer.layers.0.ln_1.weight\", \"_orig_mod.transformer.layers.0.ln_1.bias\", \"_orig_mod.transformer.layers.0.attn.c_attn.weight\", \"_orig_mod.transformer.layers.0.attn.c_attn.bias\", \"_orig_mod.transformer.layers.0.attn.c_proj.weight\", \"_orig_mod.transformer.layers.0.attn.c_proj.bias\", \"_orig_mod.transformer.layers.0.ln_2.weight\", \"_orig_mod.transformer.layers.0.ln_2.bias\", \"_orig_mod.transformer.layers.0.mlp.c_fc.weight\", \"_orig_mod.transformer.layers.0.mlp.c_fc.bias\", \"_orig_mod.transformer.layers.0.mlp.c_proj.weight\", \"_orig_mod.transformer.layers.0.mlp.c_proj.bias\", \"_orig_mod.transformer.layers.1.ln_1.weight\", \"_orig_mod.transformer.layers.1.ln_1.bias\", \"_orig_mod.transformer.layers.1.attn.c_attn.weight\", \"_orig_mod.transformer.layers.1.attn.c_attn.bias\", \"_orig_mod.transformer.layers.1.attn.c_proj.weight\", \"_orig_mod.transformer.layers.1.attn.c_proj.bias\", \"_orig_mod.transformer.layers.1.ln_2.weight\", \"_orig_mod.transformer.layers.1.ln_2.bias\", \"_orig_mod.transformer.layers.1.mlp.c_fc.weight\", \"_orig_mod.transformer.layers.1.mlp.c_fc.bias\", \"_orig_mod.transformer.layers.1.mlp.c_proj.weight\", \"_orig_mod.transformer.layers.1.mlp.c_proj.bias\", \"_orig_mod.transformer.layers.2.ln_1.weight\", \"_orig_mod.transformer.layers.2.ln_1.bias\", \"_orig_mod.transformer.layers.2.attn.c_attn.weight\", \"_orig_mod.transformer.layers.2.attn.c_attn.bias\", \"_orig_mod.transformer.layers.2.attn.c_proj.weight\", \"_orig_mod.transformer.layers.2.attn.c_proj.bias\", \"_orig_mod.transformer.layers.2.ln_2.weight\", \"_orig_mod.transformer.layers.2.ln_2.bias\", \"_orig_mod.transformer.layers.2.mlp.c_fc.weight\", \"_orig_mod.transformer.layers.2.mlp.c_fc.bias\", \"_orig_mod.transformer.layers.2.mlp.c_proj.weight\", \"_orig_mod.transformer.layers.2.mlp.c_proj.bias\", \"_orig_mod.transformer.layers.3.ln_1.weight\", \"_orig_mod.transformer.layers.3.ln_1.bias\", \"_orig_mod.transformer.layers.3.attn.c_attn.weight\", \"_orig_mod.transformer.layers.3.attn.c_attn.bias\", \"_orig_mod.transformer.layers.3.attn.c_proj.weight\", \"_orig_mod.transformer.layers.3.attn.c_proj.bias\", \"_orig_mod.transformer.layers.3.ln_2.weight\", \"_orig_mod.transformer.layers.3.ln_2.bias\", \"_orig_mod.transformer.layers.3.mlp.c_fc.weight\", \"_orig_mod.transformer.layers.3.mlp.c_fc.bias\", \"_orig_mod.transformer.layers.3.mlp.c_proj.weight\", \"_orig_mod.transformer.layers.3.mlp.c_proj.bias\", \"_orig_mod.transformer.layers.4.ln_1.weight\", \"_orig_mod.transformer.layers.4.ln_1.bias\", \"_orig_mod.transformer.layers.4.attn.c_attn.weight\", \"_orig_mod.transformer.layers.4.attn.c_attn.bias\", \"_orig_mod.transformer.layers.4.attn.c_proj.weight\", \"_orig_mod.transformer.layers.4.attn.c_proj.bias\", \"_orig_mod.transformer.layers.4.ln_2.weight\", \"_orig_mod.transformer.layers.4.ln_2.bias\", \"_orig_mod.transformer.layers.4.mlp.c_fc.weight\", \"_orig_mod.transformer.layers.4.mlp.c_fc.bias\", \"_orig_mod.transformer.layers.4.mlp.c_proj.weight\", \"_orig_mod.transformer.layers.4.mlp.c_proj.bias\", \"_orig_mod.transformer.layers.5.ln_1.weight\", \"_orig_mod.transformer.layers.5.ln_1.bias\", \"_orig_mod.transformer.layers.5.attn.c_attn.weight\", \"_orig_mod.transformer.layers.5.attn.c_attn.bias\", \"_orig_mod.transformer.layers.5.attn.c_proj.weight\", \"_orig_mod.transformer.layers.5.attn.c_proj.bias\", \"_orig_mod.transformer.layers.5.ln_2.weight\", \"_orig_mod.transformer.layers.5.ln_2.bias\", \"_orig_mod.transformer.layers.5.mlp.c_fc.weight\", \"_orig_mod.transformer.layers.5.mlp.c_fc.bias\", \"_orig_mod.transformer.layers.5.mlp.c_proj.weight\", \"_orig_mod.transformer.layers.5.mlp.c_proj.bias\", \"_orig_mod.transformer.layers.6.ln_1.weight\", \"_orig_mod.transformer.layers.6.ln_1.bias\", \"_orig_mod.transformer.layers.6.attn.c_attn.weight\", \"_orig_mod.transformer.layers.6.attn.c_attn.bias\", \"_orig_mod.transformer.layers.6.attn.c_proj.weight\", \"_orig_mod.transformer.layers.6.attn.c_proj.bias\", \"_orig_mod.transformer.layers.6.ln_2.weight\", \"_orig_mod.transformer.layers.6.ln_2.bias\", \"_orig_mod.transformer.layers.6.mlp.c_fc.weight\", \"_orig_mod.transformer.layers.6.mlp.c_fc.bias\", \"_orig_mod.transformer.layers.6.mlp.c_proj.weight\", \"_orig_mod.transformer.layers.6.mlp.c_proj.bias\", \"_orig_mod.transformer.layers.7.ln_1.weight\", \"_orig_mod.transformer.layers.7.ln_1.bias\", \"_orig_mod.transformer.layers.7.attn.c_attn.weight\", \"_orig_mod.transformer.layers.7.attn.c_attn.bias\", \"_orig_mod.transformer.layers.7.attn.c_proj.weight\", \"_orig_mod.transformer.layers.7.attn.c_proj.bias\", \"_orig_mod.transformer.layers.7.ln_2.weight\", \"_orig_mod.transformer.layers.7.ln_2.bias\", \"_orig_mod.transformer.layers.7.mlp.c_fc.weight\", \"_orig_mod.transformer.layers.7.mlp.c_fc.bias\", \"_orig_mod.transformer.layers.7.mlp.c_proj.weight\", \"_orig_mod.transformer.layers.7.mlp.c_proj.bias\", \"_orig_mod.transformer.layers.8.ln_1.weight\", \"_orig_mod.transformer.layers.8.ln_1.bias\", \"_orig_mod.transformer.layers.8.attn.c_attn.weight\", \"_orig_mod.transformer.layers.8.attn.c_attn.bias\", \"_orig_mod.transformer.layers.8.attn.c_proj.weight\", \"_orig_mod.transformer.layers.8.attn.c_proj.bias\", \"_orig_mod.transformer.layers.8.ln_2.weight\", \"_orig_mod.transformer.layers.8.ln_2.bias\", \"_orig_mod.transformer.layers.8.mlp.c_fc.weight\", \"_orig_mod.transformer.layers.8.mlp.c_fc.bias\", \"_orig_mod.transformer.layers.8.mlp.c_proj.weight\", \"_orig_mod.transformer.layers.8.mlp.c_proj.bias\", \"_orig_mod.transformer.layers.9.ln_1.weight\", \"_orig_mod.transformer.layers.9.ln_1.bias\", \"_orig_mod.transformer.layers.9.attn.c_attn.weight\", \"_orig_mod.transformer.layers.9.attn.c_attn.bias\", \"_orig_mod.transformer.layers.9.attn.c_proj.weight\", \"_orig_mod.transformer.layers.9.attn.c_proj.bias\", \"_orig_mod.transformer.layers.9.ln_2.weight\", \"_orig_mod.transformer.layers.9.ln_2.bias\", \"_orig_mod.transformer.layers.9.mlp.c_fc.weight\", \"_orig_mod.transformer.layers.9.mlp.c_fc.bias\", \"_orig_mod.transformer.layers.9.mlp.c_proj.weight\", \"_orig_mod.transformer.layers.9.mlp.c_proj.bias\", \"_orig_mod.decoder.weight\", \"_orig_mod.decoder.bias\", \"_orig_mod.projection_head.0.weight\", \"_orig_mod.projection_head.0.bias\", \"_orig_mod.projection_head.2.weight\", \"_orig_mod.projection_head.2.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(segmentation_model)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Load the trained model\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[43msegmentation_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msegmentation_model_mini_cubes.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Select a few samples from training and validation datasets\u001b[39;00m\n\u001b[0;32m     23\u001b[0m segmentation_model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[1;32mc:\\Users\\ariel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2152\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2147\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2148\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2149\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[0;32m   2151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2152\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2153\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   2154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for SegmentationModel:\n\tMissing key(s) in state_dict: \"positional_embedding\", \"summarizer\", \"tokenizer.tokenizer.weight\", \"tokenizer.tokenizer.bias\", \"transformer.layers.0.ln_1.weight\", \"transformer.layers.0.ln_1.bias\", \"transformer.layers.0.attn.c_attn.weight\", \"transformer.layers.0.attn.c_attn.bias\", \"transformer.layers.0.attn.c_proj.weight\", \"transformer.layers.0.attn.c_proj.bias\", \"transformer.layers.0.ln_2.weight\", \"transformer.layers.0.ln_2.bias\", \"transformer.layers.0.mlp.c_fc.weight\", \"transformer.layers.0.mlp.c_fc.bias\", \"transformer.layers.0.mlp.c_proj.weight\", \"transformer.layers.0.mlp.c_proj.bias\", \"transformer.layers.1.ln_1.weight\", \"transformer.layers.1.ln_1.bias\", \"transformer.layers.1.attn.c_attn.weight\", \"transformer.layers.1.attn.c_attn.bias\", \"transformer.layers.1.attn.c_proj.weight\", \"transformer.layers.1.attn.c_proj.bias\", \"transformer.layers.1.ln_2.weight\", \"transformer.layers.1.ln_2.bias\", \"transformer.layers.1.mlp.c_fc.weight\", \"transformer.layers.1.mlp.c_fc.bias\", \"transformer.layers.1.mlp.c_proj.weight\", \"transformer.layers.1.mlp.c_proj.bias\", \"transformer.layers.2.ln_1.weight\", \"transformer.layers.2.ln_1.bias\", \"transformer.layers.2.attn.c_attn.weight\", \"transformer.layers.2.attn.c_attn.bias\", \"transformer.layers.2.attn.c_proj.weight\", \"transformer.layers.2.attn.c_proj.bias\", \"transformer.layers.2.ln_2.weight\", \"transformer.layers.2.ln_2.bias\", \"transformer.layers.2.mlp.c_fc.weight\", \"transformer.layers.2.mlp.c_fc.bias\", \"transformer.layers.2.mlp.c_proj.weight\", \"transformer.layers.2.mlp.c_proj.bias\", \"transformer.layers.3.ln_1.weight\", \"transformer.layers.3.ln_1.bias\", \"transformer.layers.3.attn.c_attn.weight\", \"transformer.layers.3.attn.c_attn.bias\", \"transformer.layers.3.attn.c_proj.weight\", \"transformer.layers.3.attn.c_proj.bias\", \"transformer.layers.3.ln_2.weight\", \"transformer.layers.3.ln_2.bias\", \"transformer.layers.3.mlp.c_fc.weight\", \"transformer.layers.3.mlp.c_fc.bias\", \"transformer.layers.3.mlp.c_proj.weight\", \"transformer.layers.3.mlp.c_proj.bias\", \"transformer.layers.4.ln_1.weight\", \"transformer.layers.4.ln_1.bias\", \"transformer.layers.4.attn.c_attn.weight\", \"transformer.layers.4.attn.c_attn.bias\", \"transformer.layers.4.attn.c_proj.weight\", \"transformer.layers.4.attn.c_proj.bias\", \"transformer.layers.4.ln_2.weight\", \"transformer.layers.4.ln_2.bias\", \"transformer.layers.4.mlp.c_fc.weight\", \"transformer.layers.4.mlp.c_fc.bias\", \"transformer.layers.4.mlp.c_proj.weight\", \"transformer.layers.4.mlp.c_proj.bias\", \"transformer.layers.5.ln_1.weight\", \"transformer.layers.5.ln_1.bias\", \"transformer.layers.5.attn.c_attn.weight\", \"transformer.layers.5.attn.c_attn.bias\", \"transformer.layers.5.attn.c_proj.weight\", \"transformer.layers.5.attn.c_proj.bias\", \"transformer.layers.5.ln_2.weight\", \"transformer.layers.5.ln_2.bias\", \"transformer.layers.5.mlp.c_fc.weight\", \"transformer.layers.5.mlp.c_fc.bias\", \"transformer.layers.5.mlp.c_proj.weight\", \"transformer.layers.5.mlp.c_proj.bias\", \"transformer.layers.6.ln_1.weight\", \"transformer.layers.6.ln_1.bias\", \"transformer.layers.6.attn.c_attn.weight\", \"transformer.layers.6.attn.c_attn.bias\", \"transformer.layers.6.attn.c_proj.weight\", \"transformer.layers.6.attn.c_proj.bias\", \"transformer.layers.6.ln_2.weight\", \"transformer.layers.6.ln_2.bias\", \"transformer.layers.6.mlp.c_fc.weight\", \"transformer.layers.6.mlp.c_fc.bias\", \"transformer.layers.6.mlp.c_proj.weight\", \"transformer.layers.6.mlp.c_proj.bias\", \"transformer.layers.7.ln_1.weight\", \"transformer.layers.7.ln_1.bias\", \"transformer.layers.7.attn.c_attn.weight\", \"transformer.layers.7.attn.c_attn.bias\", \"transformer.layers.7.attn.c_proj.weight\", \"transformer.layers.7.attn.c_proj.bias\", \"transformer.layers.7.ln_2.weight\", \"transformer.layers.7.ln_2.bias\", \"transformer.layers.7.mlp.c_fc.weight\", \"transformer.layers.7.mlp.c_fc.bias\", \"transformer.layers.7.mlp.c_proj.weight\", \"transformer.layers.7.mlp.c_proj.bias\", \"transformer.layers.8.ln_1.weight\", \"transformer.layers.8.ln_1.bias\", \"transformer.layers.8.attn.c_attn.weight\", \"transformer.layers.8.attn.c_attn.bias\", \"transformer.layers.8.attn.c_proj.weight\", \"transformer.layers.8.attn.c_proj.bias\", \"transformer.layers.8.ln_2.weight\", \"transformer.layers.8.ln_2.bias\", \"transformer.layers.8.mlp.c_fc.weight\", \"transformer.layers.8.mlp.c_fc.bias\", \"transformer.layers.8.mlp.c_proj.weight\", \"transformer.layers.8.mlp.c_proj.bias\", \"transformer.layers.9.ln_1.weight\", \"transformer.layers.9.ln_1.bias\", \"transformer.layers.9.attn.c_attn.weight\", \"transformer.layers.9.attn.c_attn.bias\", \"transformer.layers.9.attn.c_proj.weight\", \"transformer.layers.9.attn.c_proj.bias\", \"transformer.layers.9.ln_2.weight\", \"transformer.layers.9.ln_2.bias\", \"transformer.layers.9.mlp.c_fc.weight\", \"transformer.layers.9.mlp.c_fc.bias\", \"transformer.layers.9.mlp.c_proj.weight\", \"transformer.layers.9.mlp.c_proj.bias\", \"decoder.weight\", \"decoder.bias\", \"projection_head.0.weight\", \"projection_head.0.bias\", \"projection_head.2.weight\", \"projection_head.2.bias\". \n\tUnexpected key(s) in state_dict: \"_orig_mod.positional_embedding\", \"_orig_mod.summarizer\", \"_orig_mod.tokenizer.tokenizer.weight\", \"_orig_mod.tokenizer.tokenizer.bias\", \"_orig_mod.transformer.layers.0.ln_1.weight\", \"_orig_mod.transformer.layers.0.ln_1.bias\", \"_orig_mod.transformer.layers.0.attn.c_attn.weight\", \"_orig_mod.transformer.layers.0.attn.c_attn.bias\", \"_orig_mod.transformer.layers.0.attn.c_proj.weight\", \"_orig_mod.transformer.layers.0.attn.c_proj.bias\", \"_orig_mod.transformer.layers.0.ln_2.weight\", \"_orig_mod.transformer.layers.0.ln_2.bias\", \"_orig_mod.transformer.layers.0.mlp.c_fc.weight\", \"_orig_mod.transformer.layers.0.mlp.c_fc.bias\", \"_orig_mod.transformer.layers.0.mlp.c_proj.weight\", \"_orig_mod.transformer.layers.0.mlp.c_proj.bias\", \"_orig_mod.transformer.layers.1.ln_1.weight\", \"_orig_mod.transformer.layers.1.ln_1.bias\", \"_orig_mod.transformer.layers.1.attn.c_attn.weight\", \"_orig_mod.transformer.layers.1.attn.c_attn.bias\", \"_orig_mod.transformer.layers.1.attn.c_proj.weight\", \"_orig_mod.transformer.layers.1.attn.c_proj.bias\", \"_orig_mod.transformer.layers.1.ln_2.weight\", \"_orig_mod.transformer.layers.1.ln_2.bias\", \"_orig_mod.transformer.layers.1.mlp.c_fc.weight\", \"_orig_mod.transformer.layers.1.mlp.c_fc.bias\", \"_orig_mod.transformer.layers.1.mlp.c_proj.weight\", \"_orig_mod.transformer.layers.1.mlp.c_proj.bias\", \"_orig_mod.transformer.layers.2.ln_1.weight\", \"_orig_mod.transformer.layers.2.ln_1.bias\", \"_orig_mod.transformer.layers.2.attn.c_attn.weight\", \"_orig_mod.transformer.layers.2.attn.c_attn.bias\", \"_orig_mod.transformer.layers.2.attn.c_proj.weight\", \"_orig_mod.transformer.layers.2.attn.c_proj.bias\", \"_orig_mod.transformer.layers.2.ln_2.weight\", \"_orig_mod.transformer.layers.2.ln_2.bias\", \"_orig_mod.transformer.layers.2.mlp.c_fc.weight\", \"_orig_mod.transformer.layers.2.mlp.c_fc.bias\", \"_orig_mod.transformer.layers.2.mlp.c_proj.weight\", \"_orig_mod.transformer.layers.2.mlp.c_proj.bias\", \"_orig_mod.transformer.layers.3.ln_1.weight\", \"_orig_mod.transformer.layers.3.ln_1.bias\", \"_orig_mod.transformer.layers.3.attn.c_attn.weight\", \"_orig_mod.transformer.layers.3.attn.c_attn.bias\", \"_orig_mod.transformer.layers.3.attn.c_proj.weight\", \"_orig_mod.transformer.layers.3.attn.c_proj.bias\", \"_orig_mod.transformer.layers.3.ln_2.weight\", \"_orig_mod.transformer.layers.3.ln_2.bias\", \"_orig_mod.transformer.layers.3.mlp.c_fc.weight\", \"_orig_mod.transformer.layers.3.mlp.c_fc.bias\", \"_orig_mod.transformer.layers.3.mlp.c_proj.weight\", \"_orig_mod.transformer.layers.3.mlp.c_proj.bias\", \"_orig_mod.transformer.layers.4.ln_1.weight\", \"_orig_mod.transformer.layers.4.ln_1.bias\", \"_orig_mod.transformer.layers.4.attn.c_attn.weight\", \"_orig_mod.transformer.layers.4.attn.c_attn.bias\", \"_orig_mod.transformer.layers.4.attn.c_proj.weight\", \"_orig_mod.transformer.layers.4.attn.c_proj.bias\", \"_orig_mod.transformer.layers.4.ln_2.weight\", \"_orig_mod.transformer.layers.4.ln_2.bias\", \"_orig_mod.transformer.layers.4.mlp.c_fc.weight\", \"_orig_mod.transformer.layers.4.mlp.c_fc.bias\", \"_orig_mod.transformer.layers.4.mlp.c_proj.weight\", \"_orig_mod.transformer.layers.4.mlp.c_proj.bias\", \"_orig_mod.transformer.layers.5.ln_1.weight\", \"_orig_mod.transformer.layers.5.ln_1.bias\", \"_orig_mod.transformer.layers.5.attn.c_attn.weight\", \"_orig_mod.transformer.layers.5.attn.c_attn.bias\", \"_orig_mod.transformer.layers.5.attn.c_proj.weight\", \"_orig_mod.transformer.layers.5.attn.c_proj.bias\", \"_orig_mod.transformer.layers.5.ln_2.weight\", \"_orig_mod.transformer.layers.5.ln_2.bias\", \"_orig_mod.transformer.layers.5.mlp.c_fc.weight\", \"_orig_mod.transformer.layers.5.mlp.c_fc.bias\", \"_orig_mod.transformer.layers.5.mlp.c_proj.weight\", \"_orig_mod.transformer.layers.5.mlp.c_proj.bias\", \"_orig_mod.transformer.layers.6.ln_1.weight\", \"_orig_mod.transformer.layers.6.ln_1.bias\", \"_orig_mod.transformer.layers.6.attn.c_attn.weight\", \"_orig_mod.transformer.layers.6.attn.c_attn.bias\", \"_orig_mod.transformer.layers.6.attn.c_proj.weight\", \"_orig_mod.transformer.layers.6.attn.c_proj.bias\", \"_orig_mod.transformer.layers.6.ln_2.weight\", \"_orig_mod.transformer.layers.6.ln_2.bias\", \"_orig_mod.transformer.layers.6.mlp.c_fc.weight\", \"_orig_mod.transformer.layers.6.mlp.c_fc.bias\", \"_orig_mod.transformer.layers.6.mlp.c_proj.weight\", \"_orig_mod.transformer.layers.6.mlp.c_proj.bias\", \"_orig_mod.transformer.layers.7.ln_1.weight\", \"_orig_mod.transformer.layers.7.ln_1.bias\", \"_orig_mod.transformer.layers.7.attn.c_attn.weight\", \"_orig_mod.transformer.layers.7.attn.c_attn.bias\", \"_orig_mod.transformer.layers.7.attn.c_proj.weight\", \"_orig_mod.transformer.layers.7.attn.c_proj.bias\", \"_orig_mod.transformer.layers.7.ln_2.weight\", \"_orig_mod.transformer.layers.7.ln_2.bias\", \"_orig_mod.transformer.layers.7.mlp.c_fc.weight\", \"_orig_mod.transformer.layers.7.mlp.c_fc.bias\", \"_orig_mod.transformer.layers.7.mlp.c_proj.weight\", \"_orig_mod.transformer.layers.7.mlp.c_proj.bias\", \"_orig_mod.transformer.layers.8.ln_1.weight\", \"_orig_mod.transformer.layers.8.ln_1.bias\", \"_orig_mod.transformer.layers.8.attn.c_attn.weight\", \"_orig_mod.transformer.layers.8.attn.c_attn.bias\", \"_orig_mod.transformer.layers.8.attn.c_proj.weight\", \"_orig_mod.transformer.layers.8.attn.c_proj.bias\", \"_orig_mod.transformer.layers.8.ln_2.weight\", \"_orig_mod.transformer.layers.8.ln_2.bias\", \"_orig_mod.transformer.layers.8.mlp.c_fc.weight\", \"_orig_mod.transformer.layers.8.mlp.c_fc.bias\", \"_orig_mod.transformer.layers.8.mlp.c_proj.weight\", \"_orig_mod.transformer.layers.8.mlp.c_proj.bias\", \"_orig_mod.transformer.layers.9.ln_1.weight\", \"_orig_mod.transformer.layers.9.ln_1.bias\", \"_orig_mod.transformer.layers.9.attn.c_attn.weight\", \"_orig_mod.transformer.layers.9.attn.c_attn.bias\", \"_orig_mod.transformer.layers.9.attn.c_proj.weight\", \"_orig_mod.transformer.layers.9.attn.c_proj.bias\", \"_orig_mod.transformer.layers.9.ln_2.weight\", \"_orig_mod.transformer.layers.9.ln_2.bias\", \"_orig_mod.transformer.layers.9.mlp.c_fc.weight\", \"_orig_mod.transformer.layers.9.mlp.c_fc.bias\", \"_orig_mod.transformer.layers.9.mlp.c_proj.weight\", \"_orig_mod.transformer.layers.9.mlp.c_proj.bias\", \"_orig_mod.decoder.weight\", \"_orig_mod.decoder.bias\", \"_orig_mod.projection_head.0.weight\", \"_orig_mod.projection_head.0.bias\", \"_orig_mod.projection_head.2.weight\", \"_orig_mod.projection_head.2.bias\". "
     ]
    }
   ],
   "source": [
    "def visualize_inference(inputs, labels, predictions, sample_idx):\n",
    "    \"\"\"\n",
    "    Visualize the input data, ground truth, and predictions using napari.\n",
    "    \"\"\"\n",
    "    # Create a napari viewer\n",
    "    viewer = napari.Viewer()\n",
    "\n",
    "    # Add input tomogram\n",
    "    viewer.add_image(inputs.cpu().numpy(), name=f\"Sample {sample_idx} - Input\", colormap=\"gray\")\n",
    "\n",
    "    # Add ground truth labels\n",
    "    viewer.add_labels(labels.cpu().numpy(), name=f\"Sample {sample_idx} - Ground Truth\")\n",
    "\n",
    "    # Add predictions\n",
    "    viewer.add_labels(predictions.cpu().numpy(), name=f\"Sample {sample_idx} - Predictions\")\n",
    "\n",
    "    # Start napari viewer\n",
    "    napari.run()\n",
    "print(segmentation_model)\n",
    "# Load the trained model\n",
    "segmentation_model.load_state_dict(torch.load(\"segmentation_model_mini_cubes.pth\"))\n",
    "# Select a few samples from training and validation datasets\n",
    "segmentation_model.eval()\n",
    "for i, (inputs, labels) in enumerate(val_loader):\n",
    "    if i >= 5:  # Visualize only 5 samples\n",
    "        break\n",
    "\n",
    "    inputs, labels = inputs[0].to(device), labels[0].to(device)\n",
    "    # print(inputs.shape, labels.shape)\n",
    "    labels = labels.reshape(16, 16, 16)  # Reshape to (16, 16, 16) for visualization\n",
    "    true_labels = torch.zeros_like(inputs[0]).to(torch.int64) # shape (96, 96, 96)\n",
    "    # Iterate over the (16, 16, 16) labels and assign them to (6, 6, 6) regions\n",
    "    for z in range(16):\n",
    "        for y in range(16):\n",
    "            for x in range(16):\n",
    "                true_labels[z*6:(z+1)*6, y*6:(y+1)*6, x*6:(x+1)*6] = labels[z, y, x]\n",
    "    # visualize_inference(inputs[0], true_labels, true_labels, sample_idx=i)\n",
    "\n",
    "    inputs = inputs.unsqueeze(1)  # Add channel dimension for model input\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = segmentation_model(inputs)\n",
    "        predictions = torch.argmax(outputs, dim=-1)  # Get predicted class labels\n",
    "        predictions = predictions.reshape(16, 16, 16)  # Reshape to (16, 16, 16) for visualization\n",
    "        true_predictions = torch.zeros_like(inputs[0, 0]).to(torch.int64) # shape (96, 96, 96)\n",
    "        for z in range(16):\n",
    "            for y in range(16):\n",
    "                for x in range(16):\n",
    "                    true_predictions[z*6:(z+1)*6, y*6:(y+1)*6, x*6:(x+1)*6] = predictions[z, y, x]\n",
    "\n",
    "    # Visualize the first sample in the batch\n",
    "    visualize_inference(inputs[0, 0], true_labels, true_predictions, sample_idx=i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
